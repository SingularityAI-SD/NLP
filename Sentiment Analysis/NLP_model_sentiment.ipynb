{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>its an amazing day great to have you as a partner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence\n",
       "0  its an amazing day great to have you as a partner"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = str(input(\"\"))\n",
    "df_test = pd.DataFrame([text],columns =['sentence'])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General defination of preprocessing for test data\n",
    "import unicodedata\n",
    "import string\n",
    "import textblob\n",
    "import nltk\n",
    "import contractions\n",
    "import re\n",
    "\n",
    "# remove some stopwords to capture negation in n-grams if possible\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "stop_words.remove('but')\n",
    "\n",
    "# load up a simple porter stemmer - nothing fancy\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "class Preprocess():\n",
    "    def remove_accented_chars(text):\n",
    "        a = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return a\n",
    "\n",
    "    def simple_text_preprocessor(document): \n",
    "        # lower case\n",
    "        document = str(document).lower()\n",
    "        \n",
    "        # expand contractions\n",
    "        document = contractions.fix(document)\n",
    "        \n",
    "        # remove unnecessary characters\n",
    "        document = re.sub(r'[^a-zA-Z]',r' ', document)\n",
    "        document = re.sub(r'nbsp', r'', document)\n",
    "        document = re.sub(' +', ' ', document)\n",
    "        \n",
    "        # simple porter stemming\n",
    "        document = ' '.join([ps.stem(word) for word in document.split()])\n",
    "        \n",
    "        # stopwords removal\n",
    "        document = ' '.join([word for word in document.split() if word not in stop_words])\n",
    "        \n",
    "        return document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/rishitkapoor/Documents/Python_ML/NLP/sentiment_train.csv\")\n",
    "a = np.array(df['sentence'])\n",
    "li =[]\n",
    "for i in a:\n",
    "    li.append(Preprocess.remove_accented_chars(i))\n",
    "\n",
    "df['sentence']=[x for x in li]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['sentence']], df[['label']],random_state=1,test_size=0.3,stratify=df[['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "X_train['char_count'] = X_train['sentence'].apply(len)\n",
    "X_train['word_count'] = X_train['sentence'].apply(lambda x: len(x.split()))\n",
    "X_train['word_density'] = X_train['char_count'] / (X_train['word_count']+1)\n",
    "X_train['punctuation_count'] = X_train['sentence'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "X_train['title_word_count'] = X_train['sentence'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "X_train['upper_case_word_count'] = X_train['sentence'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "\n",
    "X_test['char_count'] = X_test['sentence'].apply(len)\n",
    "X_test['word_count'] = X_test['sentence'].apply(lambda x: len(x.split()))\n",
    "X_test['word_density'] = X_test['char_count'] / (X_test['word_count']+1)\n",
    "X_test['punctuation_count'] = X_test['sentence'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "X_test['title_word_count'] = X_test['sentence'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "X_test['upper_case_word_count'] = X_test['sentence'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob\n",
    "x_train_snt_obj = X_train['sentence'].apply(lambda row: textblob.TextBlob(row).sentiment)\n",
    "X_train['Polarity'] = [obj.polarity for obj in x_train_snt_obj.values]\n",
    "X_train['Subjectivity'] = [obj.subjectivity for obj in x_train_snt_obj.values]\n",
    "\n",
    "x_test_snt_obj = X_test['sentence'].apply(lambda row: textblob.TextBlob(row).sentiment)\n",
    "X_test['Polarity'] = [obj.polarity for obj in x_test_snt_obj.values]\n",
    "X_test['Subjectivity'] = [obj.subjectivity for obj in x_test_snt_obj.values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stp = np.vectorize(Preprocess.simple_text_preprocessor)\n",
    "X_train['Clean sentence'] = stp(X_train['sentence'].values)\n",
    "X_test['Clean sentence'] = stp(X_test['sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_metadata = X_train.drop(['sentence', 'Clean sentence'], axis=1).reset_index(drop=True)\n",
    "X_test_metadata = X_test.drop(['sentence', 'Clean sentence'], axis=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer is a great tool provided by the scikit-learn library in Python. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abort</th>\n",
       "      <th>abram</th>\n",
       "      <th>absolut</th>\n",
       "      <th>absurd</th>\n",
       "      <th>academi</th>\n",
       "      <th>accent</th>\n",
       "      <th>accept</th>\n",
       "      <th>...</th>\n",
       "      <th>yip</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>yupanqui</th>\n",
       "      <th>zach</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zen</th>\n",
       "      <th>zesti</th>\n",
       "      <th>zoot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaron  abandon  abbey  abort  abram  absolut  absurd  academi  accent  \\\n",
       "0    0.0      0.0    0.0    0.0    0.0      0.0     0.0      0.0     0.0   \n",
       "1    0.0      0.0    0.0    0.0    0.0      0.0     0.0      0.0     0.0   \n",
       "2    0.0      0.0    0.0    0.0    0.0      0.0     0.0      0.0     0.0   \n",
       "3    0.0      0.0    0.0    0.0    0.0      0.0     0.0      0.0     0.0   \n",
       "4    0.0      0.0    0.0    0.0    0.0      0.0     0.0      0.0     0.0   \n",
       "\n",
       "   accept  ...  yip  young  younger  youth  yupanqui  zach  zeal  zen  zesti  \\\n",
       "0     0.0  ...  0.0    0.0      0.0    0.0       0.0   0.0   0.0  0.0    0.0   \n",
       "1     0.0  ...  0.0    0.0      0.0    0.0       0.0   0.0   0.0  0.0    0.0   \n",
       "2     0.0  ...  0.0    0.0      0.0    0.0       0.0   0.0   0.0  0.0    0.0   \n",
       "3     0.0  ...  0.0    0.0      0.0    0.0       0.0   0.0   0.0  0.0    0.0   \n",
       "4     0.0  ...  0.0    0.0      0.0    0.0       0.0   0.0   0.0  0.0    0.0   \n",
       "\n",
       "   zoot  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "\n",
       "[5 rows x 2155 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as tf_idf\n",
    "tf = tf_idf(min_df=0.0, max_df=1.0, ngram_range=(1, 1))\n",
    "X_traincv = tf.fit_transform(X_train['Clean sentence']).toarray()\n",
    "X_traincv = pd.DataFrame(X_traincv, columns=tf.get_feature_names())\n",
    "\n",
    "X_testcv = tf.transform(X_test['Clean sentence']).toarray()\n",
    "X_testcv = pd.DataFrame(X_testcv, columns=tf.get_feature_names())\n",
    "X_traincv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.feature_extraction.text import CountVectorizer\\n\\ncv = CountVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1, 1))\\nX_traincv = cv.fit_transform(X_train['Clean sentence']).toarray()\\nX_traincv = pd.DataFrame(X_traincv, columns=cv.get_feature_names())\\n\\nX_testcv = cv.transform(X_test['Clean sentence']).toarray()\\nX_testcv = pd.DataFrame(X_testcv, columns=cv.get_feature_names())\\nX_traincv.head()\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1, 1))\n",
    "X_traincv = cv.fit_transform(X_train['Clean sentence']).toarray()\n",
    "X_traincv = pd.DataFrame(X_traincv, columns=cv.get_feature_names())\n",
    "\n",
    "X_testcv = cv.transform(X_test['Clean sentence']).toarray()\n",
    "X_testcv = pd.DataFrame(X_testcv, columns=cv.get_feature_names())\n",
    "X_traincv.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>...</th>\n",
       "      <th>yip</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>yupanqui</th>\n",
       "      <th>zach</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zen</th>\n",
       "      <th>zesti</th>\n",
       "      <th>zoot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132</td>\n",
       "      <td>27</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126</td>\n",
       "      <td>25</td>\n",
       "      <td>4.846154</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.368750</td>\n",
       "      <td>0.74375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2163 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   char_count  word_count  word_density  punctuation_count  title_word_count  \\\n",
       "0          40           5      6.666667                  2                 2   \n",
       "1          35           5      5.833333                  3                 0   \n",
       "2         132          27      4.714286                  9                11   \n",
       "3          35           4      7.000000                  3                 1   \n",
       "4         126          25      4.846154                  6                 5   \n",
       "\n",
       "   upper_case_word_count  Polarity  Subjectivity  aaron  abandon  ...  yip  \\\n",
       "0                      0 -1.000000       1.00000    0.0      0.0  ...  0.0   \n",
       "1                      0  0.166667       1.00000    0.0      0.0  ...  0.0   \n",
       "2                      1  0.500000       0.60000    0.0      0.0  ...  0.0   \n",
       "3                      0  0.850000       1.00000    0.0      0.0  ...  0.0   \n",
       "4                      4  0.368750       0.74375    0.0      0.0  ...  0.0   \n",
       "\n",
       "   young  younger  youth  yupanqui  zach  zeal  zen  zesti  zoot  \n",
       "0    0.0      0.0    0.0       0.0   0.0   0.0  0.0    0.0   0.0  \n",
       "1    0.0      0.0    0.0       0.0   0.0   0.0  0.0    0.0   0.0  \n",
       "2    0.0      0.0    0.0       0.0   0.0   0.0  0.0    0.0   0.0  \n",
       "3    0.0      0.0    0.0       0.0   0.0   0.0  0.0    0.0   0.0  \n",
       "4    0.0      0.0    0.0       0.0   0.0   0.0  0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 2163 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_comb = pd.concat([X_train_metadata, X_traincv], axis=1)\n",
    "X_test_comb = pd.concat([X_test_metadata, X_testcv], axis=1)\n",
    "\n",
    "X_train_comb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "c = list(df_test.iloc[:,0])\n",
    "df_test['sentence']=[Preprocess.remove_accented_chars(c[0])]\n",
    "X_test_new = df_test[['sentence']]\n",
    "\n",
    "X_test_new['char_count'] = X_test_new['sentence'].apply(len)\n",
    "X_test_new['word_count'] = X_test_new['sentence'].apply(lambda x: len(x.split()))\n",
    "X_test_new['word_density'] = X_test_new['char_count'] / (X_test_new['word_count']+1)\n",
    "X_test_new['punctuation_count'] = X_test_new['sentence'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "X_test_new['title_word_count'] = X_test_new['sentence'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "X_test_new['upper_case_word_count'] = X_test_new['sentence'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "\n",
    "x_test_snt_obj = X_test_new['sentence'].apply(lambda row: textblob.TextBlob(row).sentiment)\n",
    "X_test_new['Polarity'] = [obj.polarity for obj in x_test_snt_obj.values]\n",
    "X_test_new['Subjectivity'] = [obj.subjectivity for obj in x_test_snt_obj.values]\n",
    "\n",
    "X_test_new['Clean sentence'] = stp(X_test_new['sentence'].values)\n",
    "\n",
    "X_test_metadata_2 = X_test_new.drop(['sentence', 'Clean sentence'], axis=1).reset_index(drop=True)\n",
    "\n",
    "X_testcv_2 = tf.transform(X_test_new['Clean sentence']).toarray()\n",
    "X_testcv_2 = pd.DataFrame(X_testcv_2, columns=tf.get_feature_names())\n",
    "\n",
    "X_test_comb_2 = pd.concat([X_test_metadata_2, X_testcv_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, multi_class='ovr', random_state=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1, random_state=1, solver='lbfgs',multi_class='ovr')\n",
    "lr.fit(X_train_comb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predictions = lr.predict(X_test_comb)\\nprint(classification_report(y_test, predictions))\\npd.DataFrame(confusion_matrix(y_test, predictions))'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''predictions = lr.predict(X_test_comb)\n",
    "print(classification_report(y_test, predictions))\n",
    "pd.DataFrame(confusion_matrix(y_test, predictions))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.99% Positive\n"
     ]
    }
   ],
   "source": [
    "pred_new = lr.predict(X_test_comb_2)\n",
    "proba = \"{:.2f}\".format((lr.predict_proba(X_test_comb_2).max())*100)\n",
    "cs = np.array(pred_new)\n",
    "if (cs[0]==1):\n",
    "    print(str(proba)+'% Positive')\n",
    "else:\n",
    "    print(str(proba)+'% Negative')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
